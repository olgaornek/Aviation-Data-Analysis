###############################################
### AVERAGE DEPARTURE DELAY TIME BY AIRLINE ###
%sql
SELECT airline, 
       COUNT(*) AS total_flights,
       ROUND(AVG(COALESCE((CAST(departure_actual AS INT) - CAST(departure_scheduled AS INT)) / 60, 0)), 0) AS avg_departure_delay_minutes
FROM temp_view_ss
GROUP BY airline
ORDER BY total_flights DESC



#############################
### CURRENT FLIGHT STATUS ###
from pyspark.sql.functions import col, when

df_spark = df_spark.withColumn(
    "flight_status",
    when(col("arrival_actual").isNull() & (col("departure_actual").isNull()), "scheduled")
    .when(col("arrival_actual").isNull() & (col("departure_actual").isNotNull()), "active")
    .when(col("arrival_actual").isNotNull(), "landed")
)

flight_status_count = df_spark.groupBy("flight_status").count()
display(flight_status_count)


#####################
### BUSIEST ROUTE ###
# ðŸš€ Combined count + display + chart in one go

from pyspark.sql.functions import col, coalesce, lit, desc

# 1. Compute departures and arrivals
departure_counts = (
    df_spark
      .filter(col("departure_airport").isNotNull())
      .groupBy("departure_airport")
      .count()
      .withColumnRenamed("departure_airport", "airport")
      .withColumnRenamed("count", "departure_count")
)
arrival_counts = (
    df_spark
      .filter(col("arrival_airport").isNotNull())
      .groupBy("arrival_airport")
      .count()
      .withColumnRenamed("arrival_airport", "airport")
      .withColumnRenamed("count", "arrival_count")
)

#2. Join & totalize
airport_counts = (
    departure_counts
      .join(arrival_counts, on="airport", how="full_outer")
      .select(
          "airport",
          coalesce(col("departure_count"), lit(0)).alias("departure_count"),
          coalesce(col("arrival_count")   , lit(0)).alias("arrival_count")
      )
      .withColumn("total_flights", col("departure_count") + col("arrival_count"))
      .orderBy(desc("total_flights"))
      .limit(10)
)

# 3. Show tables
print("=== Flight frequency per airport ===")
display(airport_counts)

print("=== Busiest airport ===")
display(airport_counts.limit(1))


###########################################
### FLIGHT FREQUENCY PER ROUTE OR REGION ###
result = (
    df_spark
      .groupBy(
          col("departure_airport").alias("starting_location"),
          col("arrival_airport").alias("destination")
      )
      .count()
      .orderBy(desc("count"))
      .limit(10)
)

display(result)

# Convert to Pandas for plotting
pdf_result = result.toPandas()

# Plot the top 10 routes in descending order
plt.figure(figsize=(12, 8))
plt.barh(pdf_result['starting_location'] + " -> " + pdf_result['destination'], pdf_result['count'])
plt.xlabel('Number of Flights')
plt.ylabel('Route')
plt.title('Top 10 Busiest Flight Routes')
plt.gca().invert_yaxis()  # Invert y-axis to show data in descending order
plt.tight_layout()
plt.show()


########################################################
### ACTIVE FLIGTH PUNCTUALITY BY AIRPORT & DIRECTION ###
from pyspark.sql.functions import when, col

# Helper function to compute punctuality
def compute_punctuality(actual_col, scheduled_col):
    return when(col(actual_col).isNull(), None) \
           .when(col(actual_col) > col(scheduled_col), "Late") \
           .when(col(actual_col) < col(scheduled_col), "Early") \
           .otherwise("On-Time")

# Convert pandas DataFrame to Spark DataFrame
df_spark = spark.createDataFrame(df)

# Add departure and arrival punctuality columns
df_spark = df_spark \
    .withColumn("departure_punctuality", compute_punctuality("departure_actual", "departure_scheduled")) \
    .withColumn("arrival_punctuality", compute_punctuality("arrival_actual", "arrival_scheduled"))

# Check schema and columns
df_spark.printSchema()
print("Columns:", df_spark.columns)

# Recreate the temp view
df_spark.createOrReplaceTempView("temp_view")

# Show via DataFrame API
display(df_spark)


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

spark = SparkSession.builder.getOrCreate()

# 1) Compute departure counts
dep_counts = spark.sql("""
  SELECT
    departure_airport   AS airport,
    departure_punctuality AS punctuality,
    flight_status,
    COUNT(*) AS count_punctuality
  FROM temp_view
  WHERE flight_status = 'active'
  GROUP BY departure_airport, departure_punctuality, flight_status
""").withColumn("direction", lit("departure"))

# 2) Compute arrival counts
arr_counts = spark.sql("""
  SELECT
    arrival_airport     AS airport,
    arrival_punctuality   AS punctuality,
    flight_status,
    COUNT(*) AS count_punctuality
  FROM temp_view
  WHERE flight_status = 'landed'
  GROUP BY arrival_airport, arrival_punctuality, flight_status
""").withColumn("direction", lit("arrival"))

# 3) Union into one DataFrame
combined = dep_counts.unionByName(arr_counts)

# 4) Register / inspect
combined.createOrReplaceTempView("punctuality_summary")
combined.printSchema()
print("Columns:", combined.columns)
display(combined)

# â€”â€” Optional: pivot & plot with pandas + matplotlib â€”â€” #

# Convert to Pandas (only if small enough)
pdf = combined.toPandas()

import matplotlib.pyplot as plt

# Pivot so each punctuality category becomes a column
pivoted = pdf.pivot_table(
    index=["airport","direction"],
    columns="punctuality",
    values="count_punctuality",
    fill_value=0
)

# Make a stacked bar chart
pivoted.plot(
    kind="bar",
    stacked=True,
    figsize=(10,6)
)
plt.ylabel("Number of Active Flights")
plt.title("Active Flight Punctuality by Airport & Direction")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()


######################
### FLEET ANALYSIS ###

# Fleet average age by country
%sql
SELECT country_name,fleet_average_age
FROM airlines_data
GROUP BY country_name, fleet_average_age;


# Fleet size by country
%sql
SELECT country_name, 
       ROUND(AVG(fleet_size), 0) AS average_fleet_size
FROM airlines_data
GROUP BY country_name


# Top 5 of the oldest airlines across the world.
#creating a new column displayed the age of each airline according to the years they were founded.
from datetime import datetime

df['date_founded'] = pd.to_datetime(df['date_founded'])
df['years_since_founded'] = (datetime.now() - df['date_founded']).dt.days // 365

#creating the table with top 5 the oldest airlines.
df_top_5_years = df.nlargest(5, 'years_since_founded')[['country_name', 'airline_name', 'type', 'years_since_founded']]
display(df_top_5_years)
